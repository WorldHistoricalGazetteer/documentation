# Multilingual Phonetic Toponym Matching Model

A memory-efficient, Student-Teacher neural network designed to learn phonetic similarity between place names across
languages.

**Note:** This is **not** a string similarity model (like Levenshtein or Jaro-Winkler). It is a **phonetic embedding
model** whose outputs are dense vectors compared using cosine similarity.

## Architecture Overview

This system **bypasses the Grapheme-to-Phoneme (G2P) bottleneck** by anchoring learning in a phonetic space and
distilling that knowledge into a text-based model. It employs a **Student-Teacher architecture**:

### The Teacher (Phonetic Encoder)

* **Input:** International Phonetic Alphabet (IPA) sequences generated by `Epitran`.
* **Features:** Converts IPA into articulatory feature vectors using `PanPhon` (e.g., voiced, nasal, rounded).
* **Model:** BiLSTM.
* **Role:** Learns a **phonetically grounded reference space** where names with similar pronunciations (e.g., *London*
  /lʌndən/ and *Londen* /lɔndən/) cluster tightly.
* **Limitation:** Can only be trained on languages supported by Epitran.

### The Student (Language-Conditioned Character Encoder)

* **Input:** Romanized text (via `anyascii`) + Language ID.
* **Model:** BiLSTM.
* **Mechanism:** At every timestep, the character embedding is concatenated with a **Language Embedding**. This allows
  the model to learn context-specific pronunciation rules (e.g., 'j' is /dʒ/ in English but /x/ in Spanish) without
  explicit G2P rules at runtime.
* **Role:** Approximates the Teacher's reference space using only text inputs.
* **Advantage:** Functions as a universal fallback for any language.

### The Hybrid Inference Model

During inference, the system employs a **gated fusion mechanism**:

1. **If IPA is available:** It uses a learned gate to dynamically weigh and fuse the Phonetic and Character embeddings.
2. **If IPA is unavailable:** The gate closes, and the model relies 100% on the Student (character) embedding.

---

## Key Features

* **Streaming Data Pipeline (HDF5):**
* Unlike standard PyTorch datasets that load vectors into RAM, this script streams training data directly from disk.
* **Memory Footprint:** Constant ~100MB RAM usage **for the data pipeline**, regardless of dataset size (GPU memory
  usage scales with batch size/model depth).
* **Language Agnostic:** Uses `anyascii` as a universal Romanization fallback, allowing the model to handle unseen
  scripts (e.g., matching Kanji to Katakana via Romanization).
* **Hard Negative Mining:** Phase 3 training specifically targets "look-alike" strings that sound different, pushing
  them apart in vector space.

---

## Installation

```bash
pip install torch h5py anyascii
# Optional but recommended for Phase 0-1 (The Teacher):
pip install epitran panphon elasticsearch

```

---

## The Training Pipeline

Training requires running the script in four distinct sequential phases. This separation ensures the Student learns a
stable phonetic space before generalising.

### Phase 0: Data Extraction

Streams data from an Elasticsearch index into an optimized HDF5 file. It pre-calculates IPA transliterations and PanPhon
features for all supported languages.

```bash
python phonetic_similarity_model.py --phase 0 --es-host localhost:9200 --index places --output data.h5

```

### Phase 1: Teacher Training (Phonetic Grounding)

Trains the **Phonetic Encoder** using Triplet Loss.

* *Input:* Only pairs where both sides have valid IPA generation.
* *Goal:* Create a high-quality phonetic reference space.

```bash
python phonetic_similarity_model.py --phase 1 --data data.h5 --output phase1.pt

```

### Phase 2: Alignment (Knowledge Distillation)

Trains the **Student (Character Encoder)** to mimic the frozen Teacher.

* *Loss:* MSE (position) + Cosine (orientation).
* *Goal:* Distill phonetic knowledge into the character model (e.g., learning that "J" + "Español" vector aligns with
  the Teacher's /x/ vector).

```bash
python phonetic_similarity_model.py --phase 2 --data data.h5 --phase1-model phase1.pt --output phase2.pt

```

### Phase 3: Generalization (Fine-Tuning)

Fine-tunes the Student on **all** data (including languages the Teacher didn't know).

* *Mechanism:* **Freezes** the Phonetic Encoder and the Fusion Gate. Only the Student updates.
* *Goal:* Improve separation of hard negatives and generalize to non-Epitran languages.

```bash
python phonetic_similarity_model.py --phase 3 --data data.h5 --phase2-model phase2.pt --output final_model.pt

```

---

## Python Inference API

The `PhoneticSimilarityModel` class wraps the complexity of the hybrid architecture and the gated fusion logic.

```python
from phonetic_similarity_model import PhoneticSimilarityModel

# Load the trained model
model = PhoneticSimilarityModel('final_model.pt', device='cpu')

# 1. Compare two specific toponyms
score = model.similarity(
    toponym1="London", lang1="en",
    toponym2="Londres", lang2="fr"
)
print(f"Similarity Score: {score:.4f}")

# 2. Get vector embedding (64-dim)
vector = model.embed("München", "de")

# 3. Batch processing (Faster)
candidates = [("Rome", "en"), ("Roma", "it"), ("Berlin", "de")]
results = model.find_similar("Roma", "es", candidates)

```

---

## Configuration

Hyperparameters are defined in the `Config` class at the top of the script. Key parameters to tune:

| Parameter           | Default | Description                                                        |
|---------------------|---------|--------------------------------------------------------------------|
| `VOCAB_SIZE`        | 10,000  | Size of character vocabulary. Stable hashing is used for overflow. |
| `PHONETIC_FEAT_DIM` | 24      | Dimension of PanPhon feature vectors (Articulatory features).      |
| `EMBED_DIM`         | 64      | Final output dimension of the embeddings.                          |
| `TRIPLET_MARGIN`    | 0.3     | Margin for contrastive loss in Phases 1 & 3.                       |

---
